\documentclass[10pt,conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

\begin{document}
\title{Characterization of turbulent flows in tokamaks}

\author{
  Julien Hu, Matthieu Masouyé and Sébastien Ollquist\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
    The goal of this project is to use machine learning to analyze GPI measures and estimate the Z-axis velocity of the structures.\par 
    To perform this task, we have implemented a convolutional neural network (CNN) to predict the speed of the gaz, on a synthetic dataset we generated ourselves and real data provided to us by the lab.\par
    We have tuned some hyperparameters to optimize the models and obtain the best results. We were able to successfully train and optimize a CNN for each dataset, and overall we were able to have good estimation of the speeds on the real datas. 
\end{abstract}

\section{Introduction}
A tokamak is a device which uses a powerful magnetic field to confine hot plasma in the shape of a torus\cite{wikipediatokamak} (See Fig. \ref{figure:geometrytokamak}). At the edges of this field, it separates from the flow in a turbulent fashion. These turbulences might create blobs of plasma, that can be ejected at high speed.\par 
Gas Puff Imaging (GPI) is a technique used to study these turbulences: it consists in injecting a puff of neutral gas in this region, which will be excited by the plasma. This will create light emission, that are then collected tangentially to the magnetic field lines, allowing imaging of the turbulent structures.(See Fig. \ref{figure:gpiimage})\par

\begin{figure}[!b]
  \centering
  \begin{minipage}[b]{0.2\textwidth}
    \includegraphics[width=\textwidth]{images/tokamak.png}
    \caption{Geometry of a tokamak\cite{tokamak}, which has the shape of a torus.}
    \label{figure:geometrytokamak}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.18\textwidth}
    \includegraphics[width=\textwidth]{images/blob.PNG}
    \caption{GPI images from the tokamak, we can see the gaz excited by the plasma, and the white dashed line is the peripheral region of the plasma.\cite{zweben}}
    \label{figure:gpiimage}
  \end{minipage}
\end{figure}


GPI analysis is an ongoing task, and can yield important results. The Swiss Plasma Center lab at EPFL was interested in the new possibilities machine learning could bring, and together we worked to test how well a neural network could analyze these images.\par
 

 
The primary objective was to measure the $z$-axis velocities of the moving, turbulent structures captured by GPI. We decided to opt for a convolutional neural network working on sequences of frames, as it seems a good approach for complex tasks and apt for image sequence analysis and velocity estimation \cite{velocitycnn}.



\section{Data and preprocessing}

\subsection{Real Data Description}
The labeled data was given to us from the lab in 9 pickle files, inside which there are the following sets:
\begin{enumerate}
  \item \texttt{shot} the shot number.
  \item \texttt{t\_window} an array of timestamps at which the frames were measured, in seconds
  \item \texttt{brt\_arr} a 3D-array (of 12x10x$l$ where $l$ is the length of \texttt{t\_window}) of G.P.I. measures, corresponding to the brightness of each 12 by 10 frames, measured in $mW/cm^2/ster$
  \item \texttt{r\_arr} the array of $r$-coordinate of each view, in meters.
  \item \texttt{z\_arr} the similar array of $z$-coordinate of each view, also in meters.
  \item \texttt{vz\_from\_wk} the vertical speed for each column of data, in $km/s$.
  \item \texttt{vz\_err\_from\_wk} the error margin in the estimation of vz
  \item \texttt{col\_r} the average $r$-coordinate of each column.
\end{enumerate}
Due to the measurement method, four specific "pixels" of \texttt{brt\_arr} are set to NaN permanently. The \texttt{r\_arr} and \texttt{z\_arr} describe the exact position of each pixel of \texttt{brt\_arr} in a 2d space, as they are not perfectly spaced. \texttt{vz\_from\_wk} will constitute the labels for the algorithm.\par
Importantly, there are 13 values in \texttt{vz\_from\_wk}, \texttt{vz\_err\_from\_wk} and \texttt{col\_r} while there are only 12 columns per frames: this is because the shear layer we want to calculate is inside one of the columns, which means this column will have flows going both ways. Thus, this column will have two speeds associated to it. This also means that \texttt{col\_r} has two identical values, as they concern the same column.\par
Note that it wasn't possible for the lab to provide good estimates of these two speeds in all datasets, and the 13th column will be set to NaN if they couldn't get an accurate estimation. 
Additionnally, the direction of the plasma relative to the shear layer is consistent: the left side always has plasma going down (i.e. negative values), while the right side always going up (i.e. positive values). \par
In total, we have access to 278 datapoints with the provided datasets.

\subsection{Synthetic Data Description}
We have decided to generate artificial data mainly because it would give us a controlled environment with no noise or imprecision in the labels, and also because it would make for a clearer model to train and analyse.\par
With the help of the lab, we generate data in the following way: we draw a large canvas (480x480), and spawn in random places gaussian arrays of varying random small sizes ( 264 arrays of 100 to 260 pixels). A ratio of these arrays are also set to negative values (around 33\%). To draw a frame, all the arrays are summed in the canvas, and values are limited to between 0 and 255 to stay in an 8 bit grayscale. Then, a smaller window at the center is taken and downsampled until it reaches the final size of 10x12, same as the real data.\par
At creation, these arrays are assigned a vertical speed given their horizontal coordinate by an hyperbolic tangent $\tanh$, with several settings to tweak its behaviour. To update their position at each iteration, the speed is simply added to their position. To get the labels, we can simply average this speed function over each column of the final window.\par
Everytime the script is run, it will generate a new folder inside \texttt{data/}, where the final frames will be put, with the labels, and various other information for debuging and for reproducibility purposes (like a speedplot or a list of settings).
At the beginning of the script there is a list of variables used as settings to tweak the behaviour of the generated dataset.
 
\begin{figure}
  \centering
  \includegraphics[scale=5]{images/comparison.png}
  \caption{Real image on the left, synthetic image on the right}
\label{comparison}
\end{figure}
 
This method gets reasonably close to how the real data looks like. You can see a side-to-side comparison in Fig. \ref{comparison}. In the precedent description, we have overseen a few simplifications in the behaviour of the synthetic dataset. For instance, the shear layer is always vertical but the real data's shear layer is slightly curved. Another important distinction is the speed of the structures are not only alongside the $z$-axis in the plasma. Simulating these aspects has been judged too complex for the stage of this project. Overall, the lab seemed satisfied with these synthetic datas.\par
In total, we generated 60 datapoints of 2000 frames each. We wanted to generate more, but we were limited by the computational power of our personal computer.
 


\subsection{Data Manipulation and Preprocessing}
We have implemented two different data loaders, that are used to organize and load real or synthetic data into the CNN.\par
Both data loaders organize the frames into 2000-frames datapoints (10x12x2000 values), and shuffle these datapoints using a fixed seed to homogenize the different scenarios. These datapoints are separated into 3 datasets: training, validation and testing sets, with default ratios of 64\%, 16\% and 20\% respectively.\par
The last transformation these datapoints need is to be converted from a 1-channel greyscale to 3-channel, to be compatible with the architectures we used. This was done simply by duplicating the single channel across the two others; this shouldn't pose any problem, because the model shouldn't depend on specific colors.\par
The differences between the two data loaders are where and how they fetch the frames, as they are stored in a different way on disk. Moreover, the dataloader working on real data has to rescale the values of the measures to be 8-bit greyscale images instead of raw brightness measures. This isn't a problem for the synthetic dataloader as the synthetic data was already generated in a convenient way.

Due to time constraints, we only could use part of all the sets for the real data: \texttt{t\_window}, \texttt{r\_arr}, \texttt{z\_arr} and \texttt{vz\_err\_from\_wk} were not taken into consideration for our algorithm. Moreover, we had to ignore datasets where the 13th column was missing as described in the real data description, as that would have meant changing our approach too much; those concerned 3 out of the 9 different files.\par
Finally, we choose to work with inputs of 2000 images, as more images was pushing the limits of our computers in terms of time and memory. The lab estimated that an entire structure would take approximately 18'000 frames to cross the image completely. 2'000 frames should ensure it moves at least one pixel, as the images have a height of 10 pixels.\par


\section{Model training}
 
\subsection{The CNN architectures}
To perform our regression task, we have decided to work on the architectures defined for video classification in PyTorch, for simplicity and efficiency: we trust the architectures have been optimized and tested, as it would be easier and safer to implement them rather than develop our own network. The architectures considered are: \textit{ResNet 3D}, \textit{ResNet Mixed Convolution} and \textit{ResNet (2+1)D} \cite{resnets}. ResNet is a classic neural network that helps solving computer vision tasks \cite{hara3dcnns}. As we have to analyze a sequence of images, this architecture is appropriate for our problem.\par
These networks consist of multiple processing layers that perform different operations to process the input, extract and recognise its key features (measurable property or characteristic), and predict the desired outputs. These layers are:
\begin{itemize}
  \item The convolutional layer, in which features from input images or feature maps are extracted by applying filters.
  \item The pooling layer, that is similar to the convolutional layer, but performs a specific function, such as taking the maximum or average value in a region, to reduce the complexity and amount of computation of the network 
  \item The fully connected layer, where the network regroups all informations from the final feature maps, and generates the output.
\end{itemize}

The main difference between the three architectures lies in the convolutional layers, where filters are applied distinctively:
\begin{itemize}
  \item ResNet 3D performs a 3D convolution, the 3D filters are convolved over both time and space dimensions.
  \item ResNet Mixed Convolution starts with 3D convolution, then in later layers, uses 2D convolution, as at higher level of abstraction, motion or temporal modeling may not be necessary.
  \item ResNet (2+1)D seperates the 3D convolution into two steps, a 2D convolution in the space dimension followed by a 1D convolution in time, to reduce the computation cost \cite{spacetimeconv}.
\end{itemize}

\subsection{The training procedure}
The main goal we want to achieve when training a model is preparing it sufficiently with generated data in order to be ready for the real application. In our case, we start training it on the synthetic data so that we can get a good impression of how well it performs. Then, we also train it on the real data, and try to optimize with both datasets.\par
The training loop consists of two main phases: the training phase, where the model is first trained from the training set. The model learns how to recognize the key features, and will update its weights (learnable parameters) with its performance on the dataset. Then, during the validation phase, the model simply predicts the output for the validation set, and computes the loss. This part is to give an estimation how good the model performs at each iteration, to detect any signs of overfitting. \par

Each step is repeated for 30 epochs. The last validation results are used to compare different instances of the model and choose the best hyperparameters. After optimizing and selecting the best performing model, it is then tested on the testing set, which contains data the model has never been executed with. The testing phase is only to provide us with an evaluation of the final model.\par


\subsection{Finding the correct architecture}
To find the most suitable model for our task, we decide to train and evaluate with our synthetic dataset. We see from the last validation results from table \ref{table:comparisonLoss}, that ResNet 3D offers the best results. Moreover, while training the models afterwards on the real dataset, our computers were unable to handle the ResNet MixedConv and ResNet 2+1D, as the memory load is too heavy with some set of hyperparameters, and would restrain us for optimization. We therefore select and optimize ResNet 3D. 

\begin{table}[h!]
\centering
 \begin{tabular}{|c | c |} 
 \hline
    Architecture & Validation loss \\  
 \hline\hline
    ResNet 3D & 1.181\\ 
 \hline
    ResNet MixedConv & 3.792\\
 \hline
    ResNet 2+1D & 3.284\\
 \hline
\end{tabular}
\caption{Last validation results for the architectures after 30 epochs.}
\label{table:comparisonLoss}
\end{table}


After choosing the architecture to work on, we also need to get the optimal hyperparameters. The hyperparameters we choose to improve the model with are :
\begin{itemize}
  \item The learning rate \textit{lr}, which describes how fast the model adapts to the problem, how much should it update its weights based on the estimated error. Choosing a learning rate that is too high can cause the model to converge too quickly to a sub-optimal solution or overfit the training data, while a too low learning rate results in a long training process, or can even cause the model to be stuck.
  \item \textit{batch\_size}, which defines the number of samples that will be trained at the same time, and affects how the gradient is calculated when correcting the weights. In short, bigger batches means the gradient should fluctuate less and be more accurate, at the expense of memory required. But it doesn't necessarily means a better model, as a noisier gradient might lead to a more robust model.
\end{itemize}
 
%The smaller the batch the less accurate the estimate of the gradient will be. In the figure below, you can see that the direction of the mini-batch gradient (green color) fluctuates much more in comparison to the direction of the full batch gradient (blue color). 
The loss function used to evaluate the model is the MSE (Mean Square Error) loss function, as it is the most commonly used function for regression tasks. As for the optimizer, we decide to choose the optimizer SGD. We tested with Adam, but the model struggled to converge, and resulted in worse losses. We also use a scheduler, which will cause the learning rate to decrease after some epochs, so that the model converges safely. The $\gamma$ has been set to 0.1 and the step\_size to 10, as we didn't have time to optimize them. All the seeds are the same for all the models.


%Possible que la validation loss soit meilleure au début du training qu'à la fin, mais sur le graph, on peut voir que la training loss est quand meme tres haute, et que la validation est instable au début et donc pas représentatif d'une bonne performance du modèle.
\section{Results}

\begin{table}[h]
    \centering
    \begin{tabular}{|c || c | c | c | c | c | c| c | c |} 
        \hline
           & 0.01 & 0.02 & 0.03 & 0.04 & 0.05 & 0.06 \\  
        \hline\hline
        2  & 2.81 & 1.50 & 2.54 &      &      &\\ 
        \hline
        4  & 2.74 & 1.25 & 1.05 & 1.59 &      &\\
        \hline
        8  &      & 3.77 & 1.16 & 1.02 & 2.48 &\\
        \hline
        16 &      & 5.76 & 4.02 & 3.97 & 3.12 & 4.00\\
        \hline
    \end{tabular}
    \caption{Batch size (column) vs learning rate (rows) on synthetic data}
\label{table:syntheticDataResults}
\end{table}
\begin{table}[h]
    \centering
     \begin{tabular}{|c || c | c | c | c | c | c| } 
     \hline
        & 0.001& 0.005 & 0.01  & 0.02 & 0.03 & 0.04 \\  
     \hline\hline
     2  &      &       & 7.653 & 2.394& 1.863& 3.089 \\ 
     \hline
     4  &      &       & 0.377 & 0.268& 0.256&  \\
     \hline
     8  & 0.213&  0.093& 0.111 & 0.133& 0.247&  \\
     \hline
     16 & 0.153&  0.116& 0.126 & 0.375&      &  \\
     \hline
    \end{tabular}
    \caption{Batch size (column) vs learning rate (rows) on the real data}
\label{table:realDataResults}
\end{table}


You can find the results of our models on the synthetic dataset in table \ref{table:syntheticDataResults}, and on real data in table \ref{table:realDataResults}. The losses and models are from the last epoch of the algorithm. Not all cases have been tested; we stopped increasing the learning rate when the loss was getting worse, and similarly when decreasing it. As for the batch size, we are limited by our hardware; 16 was the most we run with.\par
It is also important to note that the losses of both datasets are not directly comparable; indeed, the synthetic dataset and real dataset don't use the same metrics for measuring speed in their labels.\par

\subsection{Synthetic Dataset}
The losses throughout the epoch are a bit unstable: they often oscillate while converging, and only calm down after the decrease of the learning rate, at the tenth epoch; a good example of this is in Fig \ref{figure:oscillations}. We think this is due to both the fact that we don't have that many datapoints, and the fact that the different datasets have too many variations between them; the model has trouble learning it all. \par

\begin{figure}[h]
  \centering
  \begin{minipage}[h]{0.23\textwidth}
    \includegraphics[width=\textwidth]{images/oscillations.png}
    \caption{Training and validation losses on synthetic dataset with learning rate 0.06 and batch size 16, we observe some oscillation until epoch 10.}
    \label{figure:oscillations}
  \end{minipage}
  \hfill
  \begin{minipage}[h]{0.23\textwidth}
    \includegraphics[width=\textwidth]{images/example_high_lr.png}
    \caption{Training and validation losses on synthetic dataset with learning rate 0.03 and batch size 2, a learning rate too high means the gradient is too big, and the model has trouble finding the optimal parameters}
    \label{figure:highlr}
  \end{minipage}
\end{figure}

Another sign that our synthetic dataset is too complex for the amount of datapoint is that the model does worse with a high batch size; Higher batch size tends to degrade a model's ability to generalize, and 16 is close to half the datapoints available for training.
\subsection{Real Dataset}
The real dataset exhibits a strange behavior at higher batch sizes; Indeed, the validation loss starts going under the training loss, as you can see in Fig \ref{figure:validation}. We don't have a definite explanation, but our theory is that since there are only 6 different labels for all the real dataset, it can optimize all of them at once (on average) when the batch size is more than 6; it will find the gradient that maximize all the different labels instead of only a few at a time. This means it will be more robust, and achieve a better validation, but it will have a harder time fitting all the training example at once. This is supported by the fact that the validation loss is instantly under the train loss, while the train loss takes a while to go down; the gradient is going in a very robust direction since epoch 1.\par
Another theory is that the validation set has easier examples than the training set; but it is not consistent with the batch size; it should happen on all batch sizes.\par
Finally it might be an oversight in the loss calculation on our part; maybe we forgot a factor somewhere. It's not really consistent with the rest of the results, and the quality of the predictions, an example of which is in Fig. \ref{figure:predictions}.

\begin{figure}[h]
  \centering
  \begin{minipage}[h]{0.23\textwidth}
    \includegraphics[width=\textwidth]{images/validation.png}
    \caption{Training and validation losses with learning rate 0.005 and batch size 8, unusual behavior from validation loss relative to train loss.}
    \label{figure:validation}
  \end{minipage}
  \hfill
  \begin{minipage}[h]{0.23\textwidth}
    \includegraphics[width=\textwidth]{images/predictions.png}
    \caption{Example prediction and label with learning rate 0.005 and batch size 8 on the validation set.}
    \label{figure:predictions}
  \end{minipage}
\end{figure}


\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{project2}

\end{document}