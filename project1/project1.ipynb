{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tX = np.c_[np.ones(x.shape[0]), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################\n",
    "#\n",
    "# --- Helper functions for Linear Regression\n",
    "#\n",
    "# ###############################################\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the MSE.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    return np.square(e).mean()/2\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    gradL = (-1/tx.shape[0]) * (tx.T @ e)\n",
    "    return gradL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for sy, stx in batch_iter(y, tx, batch_size, int(tx.shape[0]/batch_size)):\n",
    "            #w = w.reshape(-1, 1)\n",
    "            grad = compute_gradient(sy, stx, w)\n",
    "            loss = compute_loss(sy, stx, w)\n",
    "            w = w - gamma * grad\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression parameters initialization\n",
    "max_iters = 50\n",
    "gamma = 1\n",
    "initial_w = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "#least_squares_GD(y, tX, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.3553503916248039, w0=[-0.06109897], w1=[0.18017386]\n",
      "Gradient Descent(1/49): loss=0.3876764884472115, w0=[-0.00552936], w1=[0.17418669]\n",
      "Gradient Descent(2/49): loss=0.3636208310855436, w0=[0.03776365], w1=[0.15188645]\n",
      "Gradient Descent(3/49): loss=0.3854599756539028, w0=[0.07231498], w1=[0.13874579]\n",
      "Gradient Descent(4/49): loss=0.36317369391107407, w0=[0.09386608], w1=[0.13302566]\n",
      "Gradient Descent(5/49): loss=0.3316027657803949, w0=[0.09007352], w1=[0.13315005]\n",
      "Gradient Descent(6/49): loss=0.364958072327731, w0=[0.1025501], w1=[0.12471724]\n",
      "Gradient Descent(7/49): loss=0.3905630524517212, w0=[0.1045102], w1=[0.11458285]\n",
      "Gradient Descent(8/49): loss=0.3553540648127124, w0=[0.10453088], w1=[0.11127824]\n",
      "Gradient Descent(9/49): loss=0.35570129512442655, w0=[0.10430295], w1=[0.11385061]\n",
      "Gradient Descent(10/49): loss=0.35715661556449446, w0=[0.11066664], w1=[0.11760836]\n",
      "Gradient Descent(11/49): loss=0.3730741774920263, w0=[0.09640926], w1=[0.10017419]\n",
      "Gradient Descent(12/49): loss=0.38406875210956193, w0=[0.09925084], w1=[0.11424596]\n",
      "Gradient Descent(13/49): loss=0.3316645201767568, w0=[0.09034748], w1=[0.11050079]\n",
      "Gradient Descent(14/49): loss=0.37667644451668447, w0=[0.0883321], w1=[0.10282749]\n",
      "Gradient Descent(15/49): loss=0.32936567403869327, w0=[0.07818481], w1=[0.1045717]\n",
      "Gradient Descent(16/49): loss=0.37061816567454603, w0=[0.07759064], w1=[0.09994986]\n",
      "Gradient Descent(17/49): loss=0.39182595229469924, w0=[0.07006949], w1=[0.10247922]\n",
      "Gradient Descent(18/49): loss=0.3494481149779584, w0=[0.06475811], w1=[0.1201309]\n",
      "Gradient Descent(19/49): loss=0.3411782980457575, w0=[0.06708591], w1=[0.10927747]\n",
      "Gradient Descent(20/49): loss=0.3819674533647401, w0=[0.05720581], w1=[0.10474976]\n",
      "Gradient Descent(21/49): loss=0.36328717980326597, w0=[0.0488884], w1=[0.11145594]\n",
      "Gradient Descent(22/49): loss=0.3530168878018952, w0=[0.05296176], w1=[0.11216979]\n",
      "Gradient Descent(23/49): loss=0.3379682974770797, w0=[0.0412484], w1=[0.10446603]\n",
      "Gradient Descent(24/49): loss=0.35092607925339864, w0=[0.03671901], w1=[0.12490223]\n",
      "Gradient Descent(25/49): loss=0.36103593307391313, w0=[0.03173704], w1=[0.10615548]\n",
      "Gradient Descent(26/49): loss=0.34336140666573106, w0=[0.0264437], w1=[0.1099669]\n",
      "Gradient Descent(27/49): loss=0.3291693474092148, w0=[0.02161831], w1=[0.11429598]\n",
      "Gradient Descent(28/49): loss=0.3673087967302657, w0=[0.02246353], w1=[0.12260167]\n",
      "Gradient Descent(29/49): loss=0.3787585636512773, w0=[0.02038385], w1=[0.11173474]\n",
      "Gradient Descent(30/49): loss=0.330598306918542, w0=[0.01062599], w1=[0.09932723]\n",
      "Gradient Descent(31/49): loss=0.3369856610237589, w0=[0.00257996], w1=[0.10589468]\n",
      "Gradient Descent(32/49): loss=0.39090727169529577, w0=[0.00564651], w1=[0.11159672]\n",
      "Gradient Descent(33/49): loss=0.336099210485457, w0=[0.0004407], w1=[0.11576601]\n",
      "Gradient Descent(34/49): loss=0.3322870400457254, w0=[-0.00100538], w1=[0.10926996]\n",
      "Gradient Descent(35/49): loss=0.3690041390818331, w0=[-0.00434484], w1=[0.11101022]\n",
      "Gradient Descent(36/49): loss=0.39878274789602325, w0=[-0.02011259], w1=[0.10899832]\n",
      "Gradient Descent(37/49): loss=0.34404449991536645, w0=[-0.01234822], w1=[0.11697796]\n",
      "Gradient Descent(38/49): loss=0.3383215123556501, w0=[-0.01655634], w1=[0.12494369]\n",
      "Gradient Descent(39/49): loss=0.35464596735384135, w0=[-0.01282816], w1=[0.12643117]\n",
      "Gradient Descent(40/49): loss=0.3764330375279607, w0=[-0.02267923], w1=[0.12236186]\n",
      "Gradient Descent(41/49): loss=0.3078756925184267, w0=[-0.03158725], w1=[0.12231728]\n",
      "Gradient Descent(42/49): loss=0.3485803133827251, w0=[-0.02589623], w1=[0.12164008]\n",
      "Gradient Descent(43/49): loss=0.3290473930924458, w0=[-0.03110997], w1=[0.11820739]\n",
      "Gradient Descent(44/49): loss=0.3304861438813316, w0=[-0.04065631], w1=[0.11443842]\n",
      "Gradient Descent(45/49): loss=0.39925560030509416, w0=[-0.02764948], w1=[0.11329566]\n",
      "Gradient Descent(46/49): loss=0.39725275324728304, w0=[-0.03470552], w1=[0.1257149]\n",
      "Gradient Descent(47/49): loss=0.3414976500532375, w0=[-0.03801338], w1=[0.12238065]\n",
      "Gradient Descent(48/49): loss=0.34678175387328847, w0=[-0.04261563], w1=[0.11343075]\n",
      "Gradient Descent(49/49): loss=0.3373438583548478, w0=[-0.04965306], w1=[0.11583286]\n",
      "SGD: execution time=5.504 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.05\n",
    "batch_size = int(tX.shape[0]/1000)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "weights = sgd_ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    m, n = tx.shape[0], tx.shape[1]\n",
    "    lambda_prime = lambda_ * 2 * m * np.eye(n)\n",
    "    a = tx.T.dot(tx) + lambda_prime\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################\n",
    "#\n",
    "# --- Helper functions for Logistic Regression\n",
    "#\n",
    "# ###############################################\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Logistic function\"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"Negative log-likelihood\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    a = -y.T.dot(np.log(pred))\n",
    "    b = (1-y).T.dot(np.log(1-pred))\n",
    "    return a-b\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)\n",
    "\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\"\"\"\n",
    "    arr = sigmoid(tx.dot(w)).T[0]\n",
    "    pred = np.diag(arr)\n",
    "    S = np.multiply(pred, 1-pred)\n",
    "    return tx.T.dot(S).dot(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, hess\n",
    "\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + (lambda_ * np.square(np.linalg.norm(w)))\n",
    "    grad = 2 * lambda_ * w + calculate_gradient(y, tx, w)\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test, mean_x_test, std_x_test = standardize(tX_test)\n",
    "tX_test = np.c_[np.ones(tX_test.shape[0]), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'output.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook project1.ipynb to script\n",
      "[NbConvertApp] Writing 4519 bytes to project1.py\n"
     ]
    }
   ],
   "source": [
    "# Convert this notebook to python script by executing this cell\n",
    "!jupyter nbconvert --to script project1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
