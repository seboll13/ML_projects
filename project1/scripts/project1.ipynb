{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tX = np.c_[np.ones(y.shape[0]), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################\n",
    "#\n",
    "# --- Helper functions for Linear Regression\n",
    "#\n",
    "# ###############################################\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the MSE.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    return np.square(e).mean()/2\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    gradL = (-1/tx.shape[0]) * (tx.T @ e)\n",
    "    return gradL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        print(n_iter)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    num_batches = 100#int(len(y) / batch_size)\n",
    "    for n_iter in range(max_iters):\n",
    "        for sy, stx in batch_iter(y, tx, batch_size, int(tx.shape[0]/batch_size)):\n",
    "            #w = w.reshape(-1, 1)\n",
    "            grad = compute_gradient(sy, stx, w)\n",
    "            loss = compute_loss(sy, stx, w)\n",
    "            w = w - gamma * grad\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression parameters initialization\n",
    "max_iters = 50\n",
    "gamma = 10**(-4)\n",
    "initial_w = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "#least_squares_GD(y, tX, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.38184870542538907, w0=[-0.05770223], w1=[0.19270056]\n",
      "Gradient Descent(1/49): loss=0.36634551072041804, w0=[-0.00322107], w1=[0.17178309]\n",
      "Gradient Descent(2/49): loss=0.36222469101211685, w0=[0.03475616], w1=[0.13885514]\n",
      "Gradient Descent(3/49): loss=0.34527446649702653, w0=[0.06677186], w1=[0.13969609]\n",
      "Gradient Descent(4/49): loss=0.32274909794268153, w0=[0.08149915], w1=[0.12756728]\n",
      "Gradient Descent(5/49): loss=0.3438473473074282, w0=[0.09741957], w1=[0.12898565]\n",
      "Gradient Descent(6/49): loss=0.3409438100945276, w0=[0.10362607], w1=[0.13373293]\n",
      "Gradient Descent(7/49): loss=0.4039193118013616, w0=[0.11000043], w1=[0.11041358]\n",
      "Gradient Descent(8/49): loss=0.37797444151654325, w0=[0.10996818], w1=[0.10146787]\n",
      "Gradient Descent(9/49): loss=0.3548202544311354, w0=[0.10226579], w1=[0.09653837]\n",
      "Gradient Descent(10/49): loss=0.35500448414469543, w0=[0.10561007], w1=[0.12353253]\n",
      "Gradient Descent(11/49): loss=0.3826763843939488, w0=[0.10279432], w1=[0.09982442]\n",
      "Gradient Descent(12/49): loss=0.34745810519892334, w0=[0.09311153], w1=[0.10503913]\n",
      "Gradient Descent(13/49): loss=0.4322244709613372, w0=[0.10687163], w1=[0.11554995]\n",
      "Gradient Descent(14/49): loss=0.3597578729259924, w0=[0.08700411], w1=[0.11958521]\n",
      "Gradient Descent(15/49): loss=0.3445250190304808, w0=[0.08347108], w1=[0.10617285]\n",
      "Gradient Descent(16/49): loss=0.3477493665919256, w0=[0.07389527], w1=[0.10682991]\n",
      "Gradient Descent(17/49): loss=0.38679779290907923, w0=[0.07527745], w1=[0.09985697]\n",
      "Gradient Descent(18/49): loss=0.3600724300315955, w0=[0.06839376], w1=[0.10683547]\n",
      "Gradient Descent(19/49): loss=0.39059878659277797, w0=[0.05929253], w1=[0.09865261]\n",
      "Gradient Descent(20/49): loss=0.35323565580344557, w0=[0.04911756], w1=[0.10578206]\n",
      "Gradient Descent(21/49): loss=0.3828545309631202, w0=[0.05254355], w1=[0.10839292]\n",
      "Gradient Descent(22/49): loss=0.3079312561697728, w0=[0.04160451], w1=[0.11383624]\n",
      "Gradient Descent(23/49): loss=0.35945889355796223, w0=[0.0461429], w1=[0.11263827]\n",
      "Gradient Descent(24/49): loss=0.44874127898200755, w0=[0.03958247], w1=[0.11020617]\n",
      "Gradient Descent(25/49): loss=0.32365037968727983, w0=[0.03618523], w1=[0.11279849]\n",
      "Gradient Descent(26/49): loss=0.32828808070879245, w0=[0.03158974], w1=[0.11607845]\n",
      "Gradient Descent(27/49): loss=0.34795085076305327, w0=[0.02928728], w1=[0.10559863]\n",
      "Gradient Descent(28/49): loss=0.36833738013028633, w0=[0.02113513], w1=[0.11417083]\n",
      "Gradient Descent(29/49): loss=0.3443229961343589, w0=[0.01622186], w1=[0.11315378]\n",
      "Gradient Descent(30/49): loss=0.32827749475933093, w0=[0.01604617], w1=[0.11572196]\n",
      "Gradient Descent(31/49): loss=0.34043583737598243, w0=[0.01213056], w1=[0.10191734]\n",
      "Gradient Descent(32/49): loss=0.38107898273791674, w0=[0.00344974], w1=[0.11816554]\n",
      "Gradient Descent(33/49): loss=0.32626954554359555, w0=[-0.00692906], w1=[0.1227415]\n",
      "Gradient Descent(34/49): loss=0.34029796284582314, w0=[-0.00831103], w1=[0.12045899]\n",
      "Gradient Descent(35/49): loss=0.34144614035585197, w0=[-0.00675619], w1=[0.11097793]\n",
      "Gradient Descent(36/49): loss=0.33909807409521764, w0=[-0.00490276], w1=[0.12355178]\n",
      "Gradient Descent(37/49): loss=0.39006141535202477, w0=[-0.01462139], w1=[0.10994863]\n",
      "Gradient Descent(38/49): loss=0.36633356041814535, w0=[-0.01582302], w1=[0.11774616]\n",
      "Gradient Descent(39/49): loss=0.3403439382797929, w0=[-0.01311294], w1=[0.11380285]\n",
      "Gradient Descent(40/49): loss=0.3560280630344773, w0=[-0.01905898], w1=[0.12605282]\n",
      "Gradient Descent(41/49): loss=0.3670830951454071, w0=[-0.03117012], w1=[0.11387998]\n",
      "Gradient Descent(42/49): loss=0.35581060271455145, w0=[-0.03129966], w1=[0.1043938]\n",
      "Gradient Descent(43/49): loss=0.3634536768458672, w0=[-0.02374914], w1=[0.1245831]\n",
      "Gradient Descent(44/49): loss=0.34857945847469834, w0=[-0.0319158], w1=[0.10962182]\n",
      "Gradient Descent(45/49): loss=0.36369858103514224, w0=[-0.03462752], w1=[0.11521194]\n",
      "Gradient Descent(46/49): loss=0.3304568744484106, w0=[-0.04083163], w1=[0.11388645]\n",
      "Gradient Descent(47/49): loss=0.3356119055807446, w0=[-0.04709796], w1=[0.11110628]\n",
      "Gradient Descent(48/49): loss=0.3675413038715095, w0=[-0.04249196], w1=[0.10607546]\n",
      "Gradient Descent(49/49): loss=0.37636876031617955, w0=[-0.05252589], w1=[0.11460392]\n",
      "SGD: execution time=5.573 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.05\n",
    "batch_size = int(tX.shape[0]/1000)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (568238,30) and (31,1) not aligned: 30 (dim 1) != 31 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dac05797c2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/submission.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/MASTER/FALL_2020-21/MACHINE_LEARNING/ML_projects/project1/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (568238,30) and (31,1) not aligned: 30 (dim 1) != 31 (dim 0)"
     ]
    }
   ],
   "source": [
    "DATA_TESTPATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test, mean_x_test, std_x_test = standardize(tX_test)\n",
    "tXtest = np.c_[np.ones(tX_test.shape[0]), tX_test]\n",
    "\n",
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "y_pred = predict_labels(sgd_ws[-1], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    m, n = tx.shape[0], tx.shape[1]\n",
    "    lambda_prime = lambda_ * 2 * m * np.eye(n)\n",
    "    a = tx.T.dot(tx) + lambda_prime\n",
    "    b = tx.T.dot(y) \n",
    "    return np.linalg.solve(a, b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
