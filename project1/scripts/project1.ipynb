{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tX = np.c_[np.ones(y.shape[0]), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################\n",
    "#\n",
    "# --- Helper functions for Linear Regression\n",
    "#\n",
    "# ###############################################\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the MSE.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    return np.square(e).mean()/2\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y.reshape(-1,1) - (tx @ w)\n",
    "    gradL = (-1/tx.shape[0]) * (tx.T @ e)\n",
    "    return gradL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for sy, stx in batch_iter(y, tx, batch_size, int(tx.shape[0]/batch_size)):\n",
    "            #w = w.reshape(-1, 1)\n",
    "            grad = compute_gradient(sy, stx, w)\n",
    "            loss = compute_loss(sy, stx, w)\n",
    "            w = w - gamma * grad\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression parameters initialization\n",
    "max_iters = 50\n",
    "gamma = 10**(-4)\n",
    "initial_w = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "#least_squares_GD(y, tX, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.35702880315710955, w0=[-0.0549311], w1=[0.17784797]\n",
      "Gradient Descent(1/49): loss=0.3971825528670109, w0=[0.00230696], w1=[0.16602506]\n",
      "Gradient Descent(2/49): loss=0.35257752924119323, w0=[0.03727121], w1=[0.16012897]\n",
      "Gradient Descent(3/49): loss=0.3652310902236802, w0=[0.06990828], w1=[0.13941647]\n",
      "Gradient Descent(4/49): loss=0.3570468207351593, w0=[0.08560014], w1=[0.12242708]\n",
      "Gradient Descent(5/49): loss=0.33394304090930166, w0=[0.09437001], w1=[0.12798643]\n",
      "Gradient Descent(6/49): loss=0.383007151473455, w0=[0.1064257], w1=[0.12394586]\n",
      "Gradient Descent(7/49): loss=0.3543806525069696, w0=[0.10343885], w1=[0.11874906]\n",
      "Gradient Descent(8/49): loss=0.373336850974154, w0=[0.11165919], w1=[0.10616876]\n",
      "Gradient Descent(9/49): loss=0.3292971848086862, w0=[0.10317462], w1=[0.10832111]\n",
      "Gradient Descent(10/49): loss=0.32789924331627135, w0=[0.10340537], w1=[0.10059135]\n",
      "Gradient Descent(11/49): loss=0.37742298665424917, w0=[0.10239486], w1=[0.1061899]\n",
      "Gradient Descent(12/49): loss=0.39128085219797004, w0=[0.10088975], w1=[0.11094993]\n",
      "Gradient Descent(13/49): loss=0.3975812878587544, w0=[0.09484887], w1=[0.10040461]\n",
      "Gradient Descent(14/49): loss=0.3555561246651104, w0=[0.08305187], w1=[0.09987844]\n",
      "Gradient Descent(15/49): loss=0.37144893810015256, w0=[0.08062163], w1=[0.09290038]\n",
      "Gradient Descent(16/49): loss=0.3648724771353873, w0=[0.07598906], w1=[0.11088173]\n",
      "Gradient Descent(17/49): loss=0.3438165618336856, w0=[0.07325192], w1=[0.10970769]\n",
      "Gradient Descent(18/49): loss=0.36188113562074126, w0=[0.07487062], w1=[0.11333685]\n",
      "Gradient Descent(19/49): loss=0.40487441346660286, w0=[0.07127704], w1=[0.10822673]\n",
      "Gradient Descent(20/49): loss=0.35569491679647836, w0=[0.0542735], w1=[0.11311016]\n",
      "Gradient Descent(21/49): loss=0.332080857405156, w0=[0.04240806], w1=[0.10471215]\n",
      "Gradient Descent(22/49): loss=0.3680248391435773, w0=[0.0532761], w1=[0.1176936]\n",
      "Gradient Descent(23/49): loss=0.3346039821874933, w0=[0.04202087], w1=[0.11631746]\n",
      "Gradient Descent(24/49): loss=0.32346727361726685, w0=[0.03473599], w1=[0.11072613]\n",
      "Gradient Descent(25/49): loss=0.31429676737272766, w0=[0.034507], w1=[0.12050651]\n",
      "Gradient Descent(26/49): loss=0.3541190747310666, w0=[0.02961047], w1=[0.1111302]\n",
      "Gradient Descent(27/49): loss=0.31792157229805357, w0=[0.02414709], w1=[0.10999734]\n",
      "Gradient Descent(28/49): loss=0.36549323149698826, w0=[0.02644684], w1=[0.11625883]\n",
      "Gradient Descent(29/49): loss=0.3554701716427355, w0=[0.01679192], w1=[0.12155041]\n",
      "Gradient Descent(30/49): loss=0.3496603086191811, w0=[0.01311782], w1=[0.1110258]\n",
      "Gradient Descent(31/49): loss=0.38758537564196505, w0=[0.01300657], w1=[0.11455294]\n",
      "Gradient Descent(32/49): loss=0.3841491760091433, w0=[0.00270487], w1=[0.12032389]\n",
      "Gradient Descent(33/49): loss=0.3397456848217736, w0=[-2.57298558e-05], w1=[0.10901865]\n",
      "Gradient Descent(34/49): loss=0.3418917636797174, w0=[-0.00351587], w1=[0.12110218]\n",
      "Gradient Descent(35/49): loss=0.3506509536959952, w0=[-0.0062272], w1=[0.11168705]\n",
      "Gradient Descent(36/49): loss=0.3504987896961484, w0=[-0.01196303], w1=[0.12979187]\n",
      "Gradient Descent(37/49): loss=0.41515917231425664, w0=[-0.01358442], w1=[0.11245699]\n",
      "Gradient Descent(38/49): loss=0.3593466831066051, w0=[-0.0160358], w1=[0.11355941]\n",
      "Gradient Descent(39/49): loss=0.3746374741245254, w0=[-0.01490396], w1=[0.11439269]\n",
      "Gradient Descent(40/49): loss=0.3288317763711075, w0=[-0.02892186], w1=[0.11478963]\n",
      "Gradient Descent(41/49): loss=0.33277840819236515, w0=[-0.03300439], w1=[0.11694448]\n",
      "Gradient Descent(42/49): loss=0.3871790435237075, w0=[-0.03003092], w1=[0.11773043]\n",
      "Gradient Descent(43/49): loss=0.3715817466764694, w0=[-0.03392552], w1=[0.11343883]\n",
      "Gradient Descent(44/49): loss=0.359348511466014, w0=[-0.03495838], w1=[0.12855954]\n",
      "Gradient Descent(45/49): loss=0.3530930588140598, w0=[-0.03064074], w1=[0.11784912]\n",
      "Gradient Descent(46/49): loss=0.33506980907676787, w0=[-0.04594524], w1=[0.11125065]\n",
      "Gradient Descent(47/49): loss=0.3372764853174609, w0=[-0.04398503], w1=[0.11674798]\n",
      "Gradient Descent(48/49): loss=0.3586917848467303, w0=[-0.04447245], w1=[0.11161577]\n",
      "Gradient Descent(49/49): loss=0.33579678710745503, w0=[-0.05035138], w1=[0.12804155]\n",
      "SGD: execution time=9.901 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.05\n",
    "batch_size = int(tX.shape[0]/1000)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros((tX.shape[1], 1))\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    m, n = tx.shape[0], tx.shape[1]\n",
    "    lambda_prime = lambda_ * 2 * m * np.eye(n)\n",
    "    a = tx.T.dot(tx) + lambda_prime\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
